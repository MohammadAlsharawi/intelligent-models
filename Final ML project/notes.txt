البيانات
عندي داتا كبيرة فيها حوالي 45 ألف سجل.

الميزات (features) مثل: العمر، الوظيفة، الحالة الاجتماعية، التعليم، الرصيد، القروض، طريقة الاتصال، مدة المكالمة، عدد الحملات السابقة… إلخ.

الليبل (y) هو إذا العميل استجاب للحملة أو لا (0 أو 1).

ما كان في قيم ناقصة، لكن كان في قيم "unknown" وقررت أتركها لأنها مهمة (مثلاً: ما اتصلوا فيه من قبل).

حولت كل الأعمدة النصية (object) لأرقام باستخدام Label Encoding.

عملت StandardScaler حتى الأرقام الكبيرة والصغيرة تصير بنفس المقياس.

عملت Feature Selection واخترت أهم 15 ميزة من أصل 17.

النماذج اللي جربتها
Linear Regression و Polynomial Regression (درجة 2 كانت جيدة، درجة 3 صار في overfitting).

SVR و Logistic Regression.

جربت النماذج مرة بدون Cross Validation ومرة مع Cross Validation، والنتائج كانت أفضل مع CV.

النماذج التصنيفية
KMeans (Clustering): أفضل إعدادات كانت n_init=10, max_iter=300.

SVC: أفضل إعدادات C=10, kernel='rbf'، والدقة وصلت 89%.

Random Forest: أفضل إعدادات n_estimators=500, max_depth=15، والدقة 89%.

XGBoost: أفضل إعدادات n_estimators=100, max_depth=7, learning_rate=0.05، والدقة 90%.

النتائج
KMeans: بيقسم البيانات لمجموعات، لكن في تداخل كبير بين الفئات.

SVC: أعطى دقة عالية، لكن أحيانًا صار في انحياز للفئة الأكبر.

Random Forest: متوازن وأداءه جيد جدًا.

XGBoost: كان أفضل نموذج، أعطى أعلى دقة (90%) وتوازن ممتاز بين الفئات.

الخلاصة
البيانات نظيفة وما فيها مشاكل كبيرة.

أهم خطوة كانت تحويل النصوص لأرقام و توحيد القيم.

Cross Validation حسّن النتائج بشكل واضح.

أفضل نموذج عندي كان XGBoost لأنه أعطى أعلى دقة وأفضل توازن.